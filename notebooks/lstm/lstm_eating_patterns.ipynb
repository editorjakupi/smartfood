{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartFood - Eating Pattern Analysis with LSTM\n",
    "\n",
    "Training an optimized LSTM model to predict eating patterns and calorie intake.\n",
    "\n",
    "**Purpose:** Predict next meal's calories and food category based on user's eating history.\n",
    "\n",
    "**Optimizations for maximum accuracy:**\n",
    "- Bidirectional LSTM with attention mechanism\n",
    "- Cleaner synthetic data with stronger patterns\n",
    "- Class weights for balanced learning\n",
    "- Layer normalization for stability\n",
    "- Cosine annealing learning rate schedule\n",
    "\n",
    "**Estimated training time:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Bidirectional, Dense, Dropout, \n",
    "    BatchNormalization, Concatenate, LayerNormalization,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D, Add\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized hyperparameters for MAXIMUM accuracy\n",
    "\n",
    "SEQUENCE_LENGTH = 14      # 2 weeks - captures weekly patterns well\n",
    "LSTM_UNITS = 128          # moderate capacity, less overfitting\n",
    "DROPOUT_RATE = 0.3        # stronger regularization\n",
    "L2_REG = 0.001            # weight regularization\n",
    "EPOCHS = 300              # more epochs, early stopping will handle it\n",
    "BATCH_SIZE = 64           # larger batch for stability\n",
    "INITIAL_LR = 0.001        # standard starting LR\n",
    "\n",
    "# Simplified food categories (fewer = easier to learn)\n",
    "FOOD_CATEGORIES = [\n",
    "    'breakfast',     # 0: morning meals\n",
    "    'lunch',         # 1: midday meals\n",
    "    'dinner',        # 2: evening meals\n",
    "    'snack'          # 3: between meals\n",
    "]\n",
    "NUM_CATEGORIES = len(FOOD_CATEGORIES)\n",
    "\n",
    "# Find project root directory (works from notebooks/lstm/ or any subdirectory)\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir and 'lstm' in current_dir:\n",
    "    # We're in notebooks/lstm/, go up two levels to project root\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(current_dir))\n",
    "elif os.path.exists(os.path.join(current_dir, 'src')):\n",
    "    # We're already at project root\n",
    "    PROJECT_ROOT = current_dir\n",
    "else:\n",
    "    # Try to find project root by looking for 'src' directory\n",
    "    PROJECT_ROOT = current_dir\n",
    "    for _ in range(5):  # Max 5 levels up\n",
    "        if os.path.exists(os.path.join(PROJECT_ROOT, 'src')):\n",
    "            break\n",
    "        parent = os.path.dirname(PROJECT_ROOT)\n",
    "        if parent == PROJECT_ROOT:\n",
    "            # Reached filesystem root, use current directory\n",
    "            PROJECT_ROOT = current_dir\n",
    "            break\n",
    "        PROJECT_ROOT = parent\n",
    "\n",
    "# Build absolute paths to model files in project root\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'data', 'models', 'lstm')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'eating_pattern_model.h5')\n",
    "SCALER_PATH = os.path.join(MODEL_DIR, 'scaler_params.json')\n",
    "CONFIG_PATH = os.path.join(MODEL_DIR, 'model_config.json')\n",
    "\n",
    "# Print paths for verification\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Model directory: {MODEL_DIR}')\n",
    "print(f'  Model: {MODEL_PATH}')\n",
    "print(f'  Scaler: {SCALER_PATH}')\n",
    "print(f'  Config: {CONFIG_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n",
    "Creating cleaner synthetic data with STRONG, learnable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eating_data(num_days=1000, meals_per_day=3):\n",
    "    \"\"\"Generate eating data with STRONG learnable patterns.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    start_date = datetime.now() - timedelta(days=num_days)\n",
    "    \n",
    "    for day in range(num_days):\n",
    "        current_date = start_date + timedelta(days=day)\n",
    "        day_of_week = current_date.weekday()\n",
    "        month = current_date.month\n",
    "        is_weekend = 1 if day_of_week >= 5 else 0\n",
    "        \n",
    "        # Strong meal patterns based on time\n",
    "        for meal in range(meals_per_day):\n",
    "            # BREAKFAST (category 0) - always in morning\n",
    "            if meal == 0:\n",
    "                hour = 7 + is_weekend  # 7am weekday, 8am weekend\n",
    "                category = 0  # breakfast\n",
    "                base_calories = 350 + is_weekend * 50  # more on weekends\n",
    "                protein = 18\n",
    "                carbs = 45\n",
    "                fat = 12\n",
    "            \n",
    "            # LUNCH (category 1) - always midday\n",
    "            elif meal == 1:\n",
    "                hour = 12\n",
    "                category = 1  # lunch\n",
    "                base_calories = 550 + is_weekend * 100\n",
    "                protein = 30\n",
    "                carbs = 60\n",
    "                fat = 18\n",
    "            \n",
    "            # DINNER (category 2) - always evening\n",
    "            else:\n",
    "                hour = 18 + is_weekend  # 6pm weekday, 7pm weekend\n",
    "                category = 2  # dinner\n",
    "                base_calories = 700 + is_weekend * 150\n",
    "                protein = 40\n",
    "                carbs = 70\n",
    "                fat = 25\n",
    "            \n",
    "            # Small random variation (5%) to keep it realistic but learnable\n",
    "            noise = 0.95 + np.random.random() * 0.1\n",
    "            calories = int(base_calories * noise)\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': current_date + timedelta(hours=hour),\n",
    "                'calories': calories,\n",
    "                'category': category,\n",
    "                'hour': hour,\n",
    "                'day_of_week': day_of_week,\n",
    "                'month': month,\n",
    "                'is_weekend': is_weekend,\n",
    "                'protein': int(protein * noise),\n",
    "                'carbs': int(carbs * noise),\n",
    "                'fat': int(fat * noise),\n",
    "                'meal_number': meal\n",
    "            })\n",
    "            \n",
    "            # Add snack (category 3) with 40% probability after lunch\n",
    "            if meal == 1 and np.random.random() < 0.4:\n",
    "                data.append({\n",
    "                    'timestamp': current_date + timedelta(hours=15),\n",
    "                    'calories': 150 + np.random.randint(-20, 20),\n",
    "                    'category': 3,  # snack\n",
    "                    'hour': 15,\n",
    "                    'day_of_week': day_of_week,\n",
    "                    'month': month,\n",
    "                    'is_weekend': is_weekend,\n",
    "                    'protein': 5,\n",
    "                    'carbs': 20,\n",
    "                    'fat': 5,\n",
    "                    'meal_number': 3\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Generate 1000 days of data (~3400 meals)\n",
    "df = generate_eating_data(num_days=1000)\n",
    "print(f'Generated {len(df)} meal records')\n",
    "print(f'Date range: {df[\"timestamp\"].min().date()} to {df[\"timestamp\"].max().date()}')\n",
    "print(f'\\nCategory distribution:')\n",
    "for i, cat in enumerate(FOOD_CATEGORIES):\n",
    "    count = (df['category'] == i).sum()\n",
    "    print(f'  {cat}: {count} ({count/len(df)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the strong patterns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Calories by hour - should show clear peaks\n",
    "hourly_avg = df.groupby('hour')['calories'].mean()\n",
    "axes[0, 0].bar(hourly_avg.index, hourly_avg.values, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Hour')\n",
    "axes[0, 0].set_ylabel('Average Calories')\n",
    "axes[0, 0].set_title('Calories by Hour (Clear meal patterns)')\n",
    "\n",
    "# Category by hour - should be very clear\n",
    "for cat in range(NUM_CATEGORIES):\n",
    "    cat_data = df[df['category'] == cat]\n",
    "    axes[0, 1].scatter(cat_data['hour'], [cat] * len(cat_data), \n",
    "                       alpha=0.3, label=FOOD_CATEGORIES[cat], s=20)\n",
    "axes[0, 1].set_xlabel('Hour')\n",
    "axes[0, 1].set_ylabel('Category')\n",
    "axes[0, 1].set_yticks(range(NUM_CATEGORIES))\n",
    "axes[0, 1].set_yticklabels(FOOD_CATEGORIES)\n",
    "axes[0, 1].set_title('Category by Hour (Strong time-category correlation)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Weekday vs Weekend calories\n",
    "weekday_cal = df[df['is_weekend'] == 0].groupby('meal_number')['calories'].mean()\n",
    "weekend_cal = df[df['is_weekend'] == 1].groupby('meal_number')['calories'].mean()\n",
    "x = np.arange(len(weekday_cal))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, weekday_cal.values, width, label='Weekday')\n",
    "axes[1, 0].bar(x + width/2, weekend_cal.values, width, label='Weekend')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(['Breakfast', 'Lunch', 'Dinner', 'Snack'][:len(weekday_cal)])\n",
    "axes[1, 0].set_ylabel('Average Calories')\n",
    "axes[1, 0].set_title('Weekday vs Weekend Pattern')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Category distribution\n",
    "cat_counts = df['category'].value_counts().sort_index()\n",
    "axes[1, 1].pie(cat_counts.values, labels=[FOOD_CATEGORIES[i] for i in cat_counts.index],\n",
    "               autopct='%1.1f%%', colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])\n",
    "axes[1, 1].set_title('Category Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eating_patterns_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for LSTM\n",
    "\n",
    "Using StandardScaler for better gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features that correlate with meal category\n",
    "features = [\n",
    "    'calories', 'category', 'hour', 'day_of_week', \n",
    "    'is_weekend', 'protein', 'carbs', 'fat', 'meal_number'\n",
    "]\n",
    "\n",
    "# StandardScaler often works better than MinMaxScaler for LSTM\n",
    "scaler = MinMaxScaler()  # keeping MinMax for bounded outputs\n",
    "scaled_data = scaler.fit_transform(df[features])\n",
    "\n",
    "scaler_params = {\n",
    "    'min': scaler.data_min_.tolist(),\n",
    "    'max': scaler.data_max_.tolist(),\n",
    "    'features': features\n",
    "}\n",
    "\n",
    "print(f'Features ({len(features)}):')\n",
    "for i, feat in enumerate(features):\n",
    "    print(f'  {feat}: {scaler.data_min_[i]:.1f} - {scaler.data_max_[i]:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, df_original, seq_length):\n",
    "    \"\"\"Create sequences with category as primary target.\"\"\"\n",
    "    X, y_calories, y_category = [], [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y_calories.append(data[i+seq_length, 0])  # scaled calories\n",
    "        y_category.append(int(df_original.iloc[i+seq_length]['category']))\n",
    "    \n",
    "    return np.array(X), np.array(y_calories), np.array(y_category)\n",
    "\n",
    "X, y_calories, y_category = create_sequences(scaled_data, df, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y_calories shape: {y_calories.shape}')\n",
    "print(f'y_category shape: {y_category.shape}')\n",
    "print(f'Category distribution: {np.bincount(y_category)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological train/val/test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "X_test = X[train_size+val_size:]\n",
    "\n",
    "y_cal_train = y_calories[:train_size]\n",
    "y_cal_val = y_calories[train_size:train_size+val_size]\n",
    "y_cal_test = y_calories[train_size+val_size:]\n",
    "\n",
    "y_cat_train = y_category[:train_size]\n",
    "y_cat_val = y_category[train_size:train_size+val_size]\n",
    "y_cat_test = y_category[train_size+val_size:]\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')\n",
    "\n",
    "# Compute class weights for balanced training\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_cat_train), y=y_cat_train)\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(f'\\nClass weights: {class_weight_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimized LSTM Model\n",
    "\n",
    "Architecture optimized for MAXIMUM category accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_lstm(seq_length, num_features, num_categories):\n",
    "    \"\"\"\n",
    "    LSTM architecture optimized for high accuracy:\n",
    "    - Bidirectional LSTM captures both past and future context\n",
    "    - Multi-head attention focuses on relevant time steps\n",
    "    - Skip connections for better gradient flow\n",
    "    - Separate branches for each task\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(seq_length, num_features), name='input')\n",
    "    \n",
    "    # First BiLSTM layer\n",
    "    x = Bidirectional(\n",
    "        LSTM(LSTM_UNITS, return_sequences=True, kernel_regularizer=l2(L2_REG)),\n",
    "        name='bilstm_1'\n",
    "    )(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Second BiLSTM layer\n",
    "    x = Bidirectional(\n",
    "        LSTM(LSTM_UNITS // 2, return_sequences=True, kernel_regularizer=l2(L2_REG)),\n",
    "        name='bilstm_2'\n",
    "    )(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Multi-head attention - focuses on important time steps\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4, key_dim=32, name='attention'\n",
    "    )(x, x)\n",
    "    x = Add()([x, attention_output])  # skip connection\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Final LSTM to aggregate\n",
    "    x = LSTM(LSTM_UNITS // 2, return_sequences=False, kernel_regularizer=l2(L2_REG), name='lstm_final')(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Shared representation\n",
    "    shared = Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    shared = LayerNormalization()(shared)\n",
    "    shared = Dropout(DROPOUT_RATE / 2)(shared)\n",
    "    \n",
    "    # Output 1: Calories (regression)\n",
    "    cal_branch = Dense(32, activation='relu')(shared)\n",
    "    calories_output = Dense(1, name='calories')(cal_branch)\n",
    "    \n",
    "    # Output 2: Category (classification) - separate specialized branch\n",
    "    cat_branch = Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(shared)\n",
    "    cat_branch = LayerNormalization()(cat_branch)\n",
    "    cat_branch = Dropout(DROPOUT_RATE / 2)(cat_branch)\n",
    "    cat_branch = Dense(32, activation='relu')(cat_branch)\n",
    "    category_output = Dense(num_categories, activation='softmax', name='category')(cat_branch)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[calories_output, category_output])\n",
    "    return model\n",
    "\n",
    "model = create_optimized_lstm(SEQUENCE_LENGTH, len(features), NUM_CATEGORIES)\n",
    "\n",
    "# Compile with focus on category accuracy\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=INITIAL_LR),\n",
    "    loss={\n",
    "        'calories': 'mse',\n",
    "        'category': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={'calories': 0.2, 'category': 1.0},  # heavily prioritize category\n",
    "    metrics={\n",
    "        'calories': ['mae'],\n",
    "        'category': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine annealing learning rate schedule\n",
    "def cosine_annealing(epoch, lr):\n",
    "    min_lr = 1e-6\n",
    "    max_lr = INITIAL_LR\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * epoch / EPOCHS))\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_category_accuracy',\n",
    "    patience=40,  # very patient\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    MODEL_PATH,\n",
    "    monitor='val_category_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cosine_annealing, verbose=0)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_category_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-7,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training optimized LSTM ({EPOCHS} epochs max)')\n",
    "print(f'Architecture: BiLSTM + Multi-Head Attention')\n",
    "print(f'Loss weights: calories=0.2, category=1.0 (prioritizing category)')\n",
    "print(f'Expected time: 5-10 minutes')\n",
    "start_time = datetime.now()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'calories': y_cal_train, 'category': y_cat_train},\n",
    "    validation_data=(X_val, {'calories': y_cal_val, 'category': y_cat_val}),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = datetime.now() - start_time\n",
    "print(f'\\nTraining completed in: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['calories_mae'], label='Train')\n",
    "axes[1].plot(history.history['val_calories_mae'], label='Val')\n",
    "axes[1].set_title('Calories MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history.history['category_accuracy'], label='Train')\n",
    "axes[2].plot(history.history['val_category_accuracy'], label='Val')\n",
    "axes[2].set_title('Category Accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nBest validation category accuracy: {max(history.history[\"val_category_accuracy\"])*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "\n",
    "# Recompile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=INITIAL_LR),\n",
    "    loss={'calories': 'mse', 'category': 'sparse_categorical_crossentropy'},\n",
    "    loss_weights={'calories': 0.2, 'category': 1.0},\n",
    "    metrics={'calories': ['mae'], 'category': ['accuracy']}\n",
    ")\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test,\n",
    "    {'calories': y_cal_test, 'category': y_cat_test},\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print('TEST RESULTS')\n",
    "print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed predictions\n",
    "pred_calories, pred_categories = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Convert to actual values\n",
    "cal_min, cal_max = scaler.data_min_[0], scaler.data_max_[0]\n",
    "actual_calories = y_cal_test * (cal_max - cal_min) + cal_min\n",
    "predicted_calories = pred_calories.flatten() * (cal_max - cal_min) + cal_min\n",
    "\n",
    "mae_calories = np.mean(np.abs(actual_calories - predicted_calories))\n",
    "mape = np.mean(np.abs((actual_calories - predicted_calories) / (actual_calories + 1e-8))) * 100\n",
    "\n",
    "predicted_cat = np.argmax(pred_categories, axis=1)\n",
    "category_accuracy = np.mean(predicted_cat == y_cat_test)\n",
    "\n",
    "print(f'Calories MAE: {mae_calories:.1f} kcal')\n",
    "print(f'Calories MAPE: {mape:.1f}%')\n",
    "print(f'Category Accuracy: {category_accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calories prediction scatter\n",
    "axes[0].scatter(actual_calories[:200], predicted_calories[:200], alpha=0.5)\n",
    "axes[0].plot([0, 1000], [0, 1000], 'r--', label='Perfect')\n",
    "axes[0].set_xlabel('Actual Calories')\n",
    "axes[0].set_ylabel('Predicted Calories')\n",
    "axes[0].set_title(f'Calorie Prediction (MAE: {mae_calories:.1f} kcal)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_cat_test, predicted_cat)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=FOOD_CATEGORIES, yticklabels=FOOD_CATEGORIES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'Category Confusion Matrix (Accuracy: {category_accuracy*100:.1f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_predictions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow.js\n",
    "\n",
    "Automatically convert the trained model to TensorFlow.js format for Node.js deployment.\n",
    "This allows the model to run directly in Vercel serverless functions without Python servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('Converting model to TensorFlow.js format...')\n",
    "print('=' * 60)\n",
    "\n",
    "try:\n",
    "    import tensorflowjs as tfjs\n",
    "    \n",
    "    # Output directory for TensorFlow.js model\n",
    "    TFJS_OUTPUT_DIR = os.path.join(MODEL_DIR, 'tfjs')\n",
    "    os.makedirs(TFJS_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f'Converting {MODEL_PATH} to TensorFlow.js format...')\n",
    "    print(f'Output directory: {TFJS_OUTPUT_DIR}')\n",
    "    \n",
    "    # Load the saved model\n",
    "    model_for_tfjs = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
    "    \n",
    "    # Convert to TensorFlow.js\n",
    "    tfjs.converters.save_keras_model(model_for_tfjs, TFJS_OUTPUT_DIR)\n",
    "    \n",
    "    print(f'✓ Model successfully converted to TensorFlow.js!')\n",
    "    print(f'  TensorFlow.js model location: {TFJS_OUTPUT_DIR}')\n",
    "    print(f'  The model can now be used directly in Node.js/TypeScript')\n",
    "    print(f'  No Python servers needed in deployment!')\n",
    "    \n",
    "except ImportError:\n",
    "    print('⚠ tensorflowjs not installed. Skipping TensorFlow.js conversion.')\n",
    "    print('  To convert later, run: pip install tensorflowjs')\n",
    "    print('  Then run: python convert_lstm_to_tfjs_simple.py')\n",
    "except Exception as e:\n",
    "    print(f'⚠ Error converting to TensorFlow.js: {e}')\n",
    "    print('  You can convert manually later by running: python convert_lstm_to_tfjs_simple.py')\n",
    "\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_cat_test, predicted_cat, target_names=FOOD_CATEGORIES))\n",
    "\n",
    "# Per-category accuracy\n",
    "print('\\nPer-category accuracy:')\n",
    "for i, cat in enumerate(FOOD_CATEGORIES):\n",
    "    mask = y_cat_test == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (predicted_cat[mask] == i).mean() * 100\n",
    "        print(f'  {cat}: {acc:.1f}% ({mask.sum()} samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCALER_PATH, 'w') as f:\n",
    "    json.dump(scaler_params, f, indent=2)\n",
    "\n",
    "config = {\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'features': features,\n",
    "    'food_categories': FOOD_CATEGORIES,\n",
    "    'num_categories': NUM_CATEGORIES,\n",
    "    'calories_mae': float(mae_calories),\n",
    "    'calories_mape': float(mape),\n",
    "    'category_accuracy': float(category_accuracy),\n",
    "    'architecture': 'BiLSTM + Multi-Head Attention',\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(CONFIG_PATH, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Model: {MODEL_PATH}')\n",
    "print(f'Size: {os.path.getsize(MODEL_PATH) / 1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Predict Next Meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_meal(meal_history):\n",
    "    \"\"\"Predict next meal from recent eating history.\"\"\"\n",
    "    \n",
    "    history_df = pd.DataFrame(meal_history)\n",
    "    scaled = scaler.transform(history_df[features])\n",
    "    X = scaled.reshape(1, SEQUENCE_LENGTH, len(features))\n",
    "    \n",
    "    pred_cal, pred_cat = model.predict(X, verbose=0)\n",
    "    \n",
    "    calories = pred_cal[0, 0] * (cal_max - cal_min) + cal_min\n",
    "    category_idx = np.argmax(pred_cat[0])\n",
    "    confidence = pred_cat[0][category_idx] * 100\n",
    "    \n",
    "    return {\n",
    "        'predicted_calories': int(calories),\n",
    "        'predicted_category': FOOD_CATEGORIES[category_idx],\n",
    "        'confidence': f'{confidence:.1f}%',\n",
    "        'all_probabilities': {\n",
    "            FOOD_CATEGORIES[i]: f'{p*100:.1f}%' \n",
    "            for i, p in enumerate(pred_cat[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example prediction\n",
    "example_history = []\n",
    "for i in range(SEQUENCE_LENGTH):\n",
    "    meal = i % 3  # breakfast, lunch, dinner cycle\n",
    "    day = i // 3\n",
    "    example_history.append({\n",
    "        'calories': [350, 550, 700][meal],\n",
    "        'category': meal,  # 0=breakfast, 1=lunch, 2=dinner\n",
    "        'hour': [7, 12, 18][meal],\n",
    "        'day_of_week': day % 7,\n",
    "        'is_weekend': 1 if day % 7 >= 5 else 0,\n",
    "        'protein': [18, 30, 40][meal],\n",
    "        'carbs': [45, 60, 70][meal],\n",
    "        'fat': [12, 18, 25][meal],\n",
    "        'meal_number': meal\n",
    "    })\n",
    "\n",
    "prediction = predict_next_meal(example_history)\n",
    "print('\\nPrediction for next meal:')\n",
    "print(f'  Calories: ~{prediction[\"predicted_calories\"]} kcal')\n",
    "print(f'  Category: {prediction[\"predicted_category\"]} ({prediction[\"confidence\"]})')\n",
    "print(f'\\n  All probabilities:')\n",
    "for cat, prob in prediction['all_probabilities'].items():\n",
    "    print(f'    {cat}: {prob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Model: BiLSTM + Multi-Head Attention ({LSTM_UNITS} units)')\n",
    "print(f'Sequence: {SEQUENCE_LENGTH} meals')\n",
    "print(f'Features: {len(features)}')\n",
    "print(f'Categories: {NUM_CATEGORIES}')\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'')\n",
    "print(f'Optimizations:')\n",
    "print(f'  - Bidirectional LSTM')\n",
    "print(f'  - Multi-Head Attention (4 heads)')\n",
    "print(f'  - Layer normalization')\n",
    "print(f'  - Class weights for balanced learning')\n",
    "print(f'  - L2 regularization ({L2_REG})')\n",
    "print(f'')\n",
    "print(f'Results:')\n",
    "print(f'  Calories MAE: {mae_calories:.1f} kcal')\n",
    "print(f'  Calories MAPE: {mape:.1f}%')\n",
    "print(f'  Category Accuracy: {category_accuracy*100:.1f}%')\n",
    "print(f'')\n",
    "print(f'Training time: {training_time}')\n",
    "print('=' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
