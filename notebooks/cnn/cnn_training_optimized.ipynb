{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Food Classifier - Transfer Learning\n",
        "\n",
        "Training a food classification model using transfer learning with EfficientNetB4 pre-trained on ImageNet.\n",
        "\n",
        "**Approach**:\n",
        "- Freeze entire pre-trained base model\n",
        "- Train only the custom classifier head\n",
        "- ImageNet normalization for pre-trained weights compatibility\n",
        "- Data augmentation for regularization\n",
        "- Early stopping to prevent overfitting\n",
        "\n",
        "**Target**: 80%+ top-1 accuracy on Food-101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# Enable mixed precision for GPU memory efficiency\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(\"Mixed precision enabled: float16 computation with float32 accumulation\")\n",
        "else:\n",
        "    print(\"No GPU detected, training will use CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Optimized for EfficientNetB4 and Food-101 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "IMAGE_SIZE = (224, 224)  # Using 224 instead of 380 for faster training with minimal accuracy loss\n",
        "NUM_CLASSES = 101\n",
        "BATCH_SIZE = 20  # Optimized for EfficientNetB4 with mixed precision on GPU\n",
        "\n",
        "# Training configuration\n",
        "MAX_EPOCHS = 40  # Maximum epochs with early stopping\n",
        "LEARNING_RATE = 1e-3  # Standard LR for training new classifier from scratch\n",
        "\n",
        "# Regularization\n",
        "LABEL_SMOOTHING = 0.1  # Prevents overconfident predictions\n",
        "DROPOUT_RATE = 0.4     # Higher dropout for larger model\n",
        "\n",
        "# ImageNet normalization - required for pre-trained models\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "# Callbacks\n",
        "EARLY_STOPPING_PATIENCE = 8  # Stop if no improvement for 8 epochs\n",
        "LR_PATIENCE = 3              # Reduce LR if no improvement for 3 epochs\n",
        "LR_FACTOR = 0.5              # Reduce LR by half when plateau\n",
        "\n",
        "# Paths: work whether kernel cwd is notebooks/cnn, notebooks, or smartfood root\n",
        "_cwd = os.getcwd()\n",
        "if os.path.basename(_cwd) == 'cnn':\n",
        "    BASE_DIR = os.path.abspath(os.path.join(_cwd, '..', '..'))\n",
        "elif os.path.basename(_cwd) == 'notebooks':\n",
        "    BASE_DIR = os.path.abspath(os.path.join(_cwd, '..'))\n",
        "else:\n",
        "    BASE_DIR = os.path.abspath(_cwd)\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data', 'food-101')\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'data', 'models', 'cnn')\n",
        "MODEL_PATH = os.path.join(MODEL_DIR, 'food_classifier_best.keras')\n",
        "HISTORY_PATH = os.path.join(MODEL_DIR, 'training_history.json')\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Model: EfficientNetB4\")\n",
        "print(f\"Image size: {IMAGE_SIZE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Max epochs: {MAX_EPOCHS} (with early stopping patience {EARLY_STOPPING_PATIENCE})\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"\\nEstimated training time: 5-8 hours (with early stopping likely 15-25 epochs)\")\n",
        "print(f\"Per epoch: ~12-15 minutes\")\n",
        "print(f\"\\nData: {DATA_DIR}\")\n",
        "print(f\"Model output: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2b. Prepare train/val/test splits\n",
        "\n",
        "Creates `train/`, `val/`, and `test/` from Food-101 `images/` and `meta/` (or skips if they exist). Run once; if you see download instructions, get Food-101 and re-run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if train/val/test already exist\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "images_dir = os.path.join(DATA_DIR, 'images')\n",
        "meta_dir = os.path.join(DATA_DIR, 'meta')\n",
        "\n",
        "DATA_READY = False\n",
        "\n",
        "if os.path.isdir(train_dir) and os.path.isdir(val_dir) and os.path.isdir(test_dir):\n",
        "    print(f\"Data ready: {train_dir}, val, test found.\")\n",
        "    DATA_READY = True\n",
        "elif os.path.isdir(images_dir) and os.path.isfile(os.path.join(meta_dir, 'train.txt')) and os.path.isfile(os.path.join(meta_dir, 'test.txt')):\n",
        "    print(\"Creating train/val/test from images/ and meta/...\")\n",
        "    train_txt = os.path.join(meta_dir, 'train.txt')\n",
        "    test_txt = os.path.join(meta_dir, 'test.txt')\n",
        "    val_ratio = 0.15  # 15% of training per class for validation\n",
        "\n",
        "    def read_split(path):\n",
        "        with open(path) as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "        by_class = {}\n",
        "        for line in lines:\n",
        "            cls, _ = line.split('/')\n",
        "            by_class.setdefault(cls, []).append(line)\n",
        "        return by_class\n",
        "\n",
        "    train_by_class = read_split(train_txt)\n",
        "    test_by_class = read_split(test_txt)\n",
        "    class_names = sorted(train_by_class.keys())\n",
        "\n",
        "    for split, base in [('train', train_dir), ('val', val_dir), ('test', test_dir)]:\n",
        "        os.makedirs(base, exist_ok=True)\n",
        "        for c in class_names:\n",
        "            os.makedirs(os.path.join(base, c), exist_ok=True)\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    for cls in class_names:\n",
        "        train_list = train_by_class[cls]\n",
        "        random.shuffle(train_list)\n",
        "        n_val = max(1, int(len(train_list) * val_ratio))\n",
        "        val_list = train_list[:n_val]\n",
        "        train_list = train_list[n_val:]\n",
        "        for rel in train_list:\n",
        "            src = os.path.join(images_dir, rel + '.jpg')\n",
        "            dst = os.path.join(train_dir, rel + '.jpg')\n",
        "            if os.path.isfile(src) and not os.path.lexists(dst):\n",
        "                try:\n",
        "                    os.symlink(os.path.abspath(src), dst)\n",
        "                except OSError:\n",
        "                    import shutil\n",
        "                    shutil.copy2(src, dst)\n",
        "        for rel in val_list:\n",
        "            src = os.path.join(images_dir, rel + '.jpg')\n",
        "            dst = os.path.join(val_dir, rel + '.jpg')\n",
        "            if os.path.isfile(src) and not os.path.lexists(dst):\n",
        "                try:\n",
        "                    os.symlink(os.path.abspath(src), dst)\n",
        "                except OSError:\n",
        "                    import shutil\n",
        "                    shutil.copy2(src, dst)\n",
        "        for rel in test_by_class.get(cls, []):\n",
        "            src = os.path.join(images_dir, rel + '.jpg')\n",
        "            dst = os.path.join(test_dir, rel + '.jpg')\n",
        "            if os.path.isfile(src) and not os.path.lexists(dst):\n",
        "                try:\n",
        "                    os.symlink(os.path.abspath(src), dst)\n",
        "                except OSError:\n",
        "                    import shutil\n",
        "                    shutil.copy2(src, dst)\n",
        "    print(\"train/val/test created.\")\n",
        "    DATA_READY = True\n",
        "else:\n",
        "    print(\"Food-101 data not found. To run training:\")\n",
        "    print(\"1. Download food-101.tar.gz from: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\")\n",
        "    print(\"2. Extract so you have: data/food-101/images/ (101 class subdirs) and data/food-101/meta/ (train.txt, test.txt)\")\n",
        "    print(\"3. Re-run this cell; it will create data/food-101/train/, val/, test/ from images and meta.\")\n",
        "    print(\"Expected paths:\", train_dir, val_dir, test_dir)\n",
        "\n",
        "# Summary of train/val/test sample counts\n",
        "if DATA_READY:\n",
        "    def _count_split(d):\n",
        "        return sum(\n",
        "            len([f for f in os.listdir(os.path.join(d, c)) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            for c in sorted(os.listdir(d))\n",
        "            if os.path.isdir(os.path.join(d, c))\n",
        "        )\n",
        "    n_train = _count_split(train_dir)\n",
        "    n_val = _count_split(val_dir)\n",
        "    n_test = _count_split(test_dir)\n",
        "    n_classes = len([c for c in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, c))])\n",
        "    print(\"Train samples:\", n_train, \"| Validation:\", n_val, \"| Test:\", n_test, \"| Classes:\", n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not DATA_READY or not os.path.isdir(os.path.join(DATA_DIR, 'train')):\n",
        "    raise FileNotFoundError(\n",
        "        \"Food-101 train/val/test not found. Run the previous cell (2b. Prepare Food-101 data). \"\n",
        "        \"If you have not downloaded Food-101 yet: get food-101.tar.gz from \"\n",
        "        \"https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/ and extract so that \"\n",
        "        \"data/food-101/images/ and data/food-101/meta/ (train.txt, test.txt) exist, then re-run cell 2b.\"\n",
        "    )\n",
        "\n",
        "print(\"Loading Food-101 dataset...\\n\")\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, 'train'),\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, 'val'),\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    os.path.join(DATA_DIR, 'test'),\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "train_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "val_batches = tf.data.experimental.cardinality(val_dataset).numpy()\n",
        "test_batches = tf.data.experimental.cardinality(test_dataset).numpy()\n",
        "\n",
        "print(f\"Dataset loaded: {len(class_names)} classes\")\n",
        "print(f\"Training batches: {train_batches} (~{train_batches * BATCH_SIZE} images)\")\n",
        "print(f\"Validation batches: {val_batches}\")\n",
        "print(f\"Test batches: {test_batches}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Pipeline\n",
        "\n",
        "Preprocessing steps:\n",
        "1. Scale pixels to 0-1 range\n",
        "2. Apply augmentation (training only)\n",
        "3. Apply ImageNet normalization\n",
        "\n",
        "Augmentation is subtle to maintain visual recognizability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def preprocess(image, label):\n",
        "    \"\"\"Scale and apply ImageNet normalization.\"\"\"\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    return image, label\n",
        "\n",
        "@tf.function\n",
        "def augment(image, label):\n",
        "    \"\"\"Subtle augmentation for regularization.\"\"\"\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    image = tf.keras.layers.RandomRotation(0.03)(image)\n",
        "    image = tf.keras.layers.RandomZoom(0.05)(image)\n",
        "    image = tf.clip_by_value(image, -3.0, 3.0)\n",
        "    return image, label\n",
        "\n",
        "print(\"Building data pipelines...\")\n",
        "\n",
        "# Training pipeline: scale -> augment -> normalize -> prefetch\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: (tf.cast(x, tf.float32) / 255.0, y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: ((x - IMAGENET_MEAN) / IMAGENET_STD, y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation and test: only normalize\n",
        "val_dataset = val_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Data pipelines ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Architecture\n",
        "\n",
        "Transfer learning with EfficientNetB4:\n",
        "- Pre-trained base frozen (base.trainable = False)\n",
        "- Custom classifier head trained from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Building model...\\n\")\n",
        "\n",
        "# Load pre-trained EfficientNetB4 without top classification layers\n",
        "base_model = EfficientNetB4(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(*IMAGE_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Freeze entire base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build model with custom classifier head\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(DROPOUT_RATE),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(DROPOUT_RATE),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')\n",
        "])\n",
        "\n",
        "total_params = model.count_params()\n",
        "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "\n",
        "print(f\"Model: EfficientNetB4 + custom classifier\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable: {trainable_params:,} (classifier only)\")\n",
        "print(f\"Frozen: {total_params - trainable_params:,} (pre-trained base)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Callbacks\n",
        "\n",
        "Training optimization callbacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    MODEL_PATH,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=EARLY_STOPPING_PATIENCE,\n",
        "    mode='max',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=LR_FACTOR,\n",
        "    patience=LR_PATIENCE,\n",
        "    mode='max',\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [checkpoint, early_stop, reduce_lr]\n",
        "\n",
        "print(\"Callbacks configured:\")\n",
        "print(f\"  Model checkpoint: saves best val_accuracy\")\n",
        "print(f\"  Early stopping: patience {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "print(f\"  LR reduction: patience {LR_PATIENCE}, factor {LR_FACTOR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training\n",
        "\n",
        "Train only the classifier head with frozen base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')]\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training Classifier (Base Model Frozen)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Max epochs: {MAX_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Early stopping will trigger after {EARLY_STOPPING_PATIENCE} epochs without improvement\")\n",
        "print(f\"\\nStarting training...\\n\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=MAX_EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "training_time = datetime.now() - start_time\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "epochs_trained = len(history.history['val_accuracy'])\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training completed in {training_time}\")\n",
        "print(f\"Epochs trained: {epochs_trained}/{MAX_EPOCHS}\")\n",
        "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation\n",
        "\n",
        "Evaluate the best model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Test Set Evaluation\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "test_results = model.evaluate(test_dataset, verbose=2)\n",
        "\n",
        "test_loss = test_results[0]\n",
        "test_accuracy = test_results[1]\n",
        "test_top5 = test_results[2]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-1 Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Test Top-5 Accuracy: {test_top5*100:.2f}%\")\n",
        "print(f\"\\nTraining time: {training_time}\")\n",
        "print(f\"Epochs: {epochs_trained}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "if test_accuracy >= 0.80:\n",
        "    print(\"Target achieved: 80%+ accuracy\")\n",
        "elif test_accuracy >= 0.75:\n",
        "    print(\"Strong performance, close to target\")\n",
        "else:\n",
        "    print(\"Below target - consider longer training or hyperparameter adjustment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_history = {\n",
        "    'history': {k: [float(v) for v in values] for k, values in history.history.items()},\n",
        "    'test_results': {\n",
        "        'loss': float(test_loss),\n",
        "        'accuracy': float(test_accuracy),\n",
        "        'top5_accuracy': float(test_top5)\n",
        "    },\n",
        "    'config': {\n",
        "        'model': 'EfficientNetB4',\n",
        "        'image_size': IMAGE_SIZE,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'epochs_trained': epochs_trained,\n",
        "        'max_epochs': MAX_EPOCHS,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'label_smoothing': LABEL_SMOOTHING,\n",
        "        'dropout_rate': DROPOUT_RATE,\n",
        "        'training_time': str(training_time)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(HISTORY_PATH, 'w') as f:\n",
        "    json.dump(training_history, f, indent=2)\n",
        "\n",
        "print(f\"Training history saved: {HISTORY_PATH}\")\n",
        "print(f\"Best model saved: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. TensorFlow.js Conversion\n",
        "\n",
        "Convert for deployment in Next.js application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print('Converting to TensorFlow.js...')\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "try:\n",
        "    import tensorflowjs as tfjs\n",
        "    \n",
        "    tfjs_dir = os.path.join(MODEL_DIR, 'tfjs')\n",
        "    \n",
        "    tfjs.converters.save_keras_model(\n",
        "        model,\n",
        "        tfjs_dir,\n",
        "        quantization_dtype=None\n",
        "    )\n",
        "    \n",
        "    print(f\"TensorFlow.js model saved: {tfjs_dir}\")\n",
        "    print(\"Ready for Next.js deployment\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"tensorflowjs not installed, skipping conversion\")\n",
        "    print(\"Install: pip install tensorflowjs==3.21.0\")\n",
        "except Exception as e:\n",
        "    print(f\"Conversion failed: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
