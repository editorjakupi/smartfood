{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Model Conversion (TensorFlow.js & ONNX)\n",
        "\n",
        "Convert the trained Keras model `food_classifier_best.keras` to TensorFlow.js and/or ONNX.\n",
        "Run this notebook whenever you want to update the exported models after retraining.\n",
        "\n",
        "**Requirements:** `food_classifier_best.keras` must be in `data/models/cnn/` (same location the SmartFood app uses).\n",
        "\n",
        "You can run the **TensorFlow.js** and **ONNX** conversion cells independently of each other and in any order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paths (optional)\n",
        "\n",
        "Run this cell first to set paths (notebook is expected to live in `notebooks/cnn/`). You can also run each conversion cell below on its own; they set paths internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"data\", \"models\", \"cnn\")\n",
        "\n",
        "print(f\"MODEL_DIR: {MODEL_DIR}\")\n",
        "model_file = os.path.join(MODEL_DIR, \"food_classifier_best.keras\")\n",
        "print(f\"Keras model: {model_file}\")\n",
        "if not os.path.exists(model_file):\n",
        "    print(\"  File not found – train first or copy the model here.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to TensorFlow.js\n",
        "\n",
        "Converts the trained Keras model to TensorFlow.js for deployment in web applications. **Runs independently** – no need to run the ONNX cell or the Paths cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TensorFlow.js conversion – runs independently (sets paths if needed)\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'data', 'models', 'cnn')\n",
        "TFJS_OUTPUT_DIR = os.path.join(MODEL_DIR, 'tfjs')\n",
        "model_path = os.path.join(MODEL_DIR, 'food_classifier_best.keras')\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('Converting model to TensorFlow.js format...')\n",
        "print('=' * 60)\n",
        "\n",
        "try:\n",
        "    import tensorflowjs as tfjs\n",
        "    \n",
        "    os.makedirs(TFJS_OUTPUT_DIR, exist_ok=True)\n",
        "    \n",
        "    print(f'Converting {model_path} to TensorFlow.js format...')\n",
        "    print(f'Output directory: {TFJS_OUTPUT_DIR}')\n",
        "    \n",
        "    # Load the saved model\n",
        "    model_for_tfjs = tf.keras.models.load_model(model_path, compile=False)\n",
        "    \n",
        "    # Convert to TensorFlow.js\n",
        "    tfjs.converters.save_keras_model(model_for_tfjs, TFJS_OUTPUT_DIR)\n",
        "    \n",
        "    print(f'✓ Model successfully converted to TensorFlow.js!')\n",
        "    print(f'  TensorFlow.js model location: {TFJS_OUTPUT_DIR}')\n",
        "    print(f'  The model can now be used directly in web browsers')\n",
        "    print(f'  No Python servers needed in deployment!')\n",
        "    \n",
        "except ImportError:\n",
        "    print('⚠ tensorflowjs not installed. Skipping TensorFlow.js conversion.')\n",
        "    print('  To convert later, install: pip install --break-system-packages --timeout=300 tensorflowjs')\n",
        "    print('  Then re-run this cell or use: tensorflowjs_converter --input_format=keras model.keras output_dir/')\n",
        "except Exception as e:\n",
        "    print(f'⚠ Error converting to TensorFlow.js: {e}')\n",
        "    print('  The Keras model is still saved and can be converted manually later.')\n",
        "\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to ONNX\n",
        "\n",
        "Exports the model to ONNX so the Next.js app can use `onnxruntime-node` for faster inference than TensorFlow.js CPU. **Runs independently** – no need to run the TensorFlow.js cell or the Paths cell first. Requires: `pip install tf2onnx onnx`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ONNX conversion - runs independently\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "MODEL_DIR = os.path.join(BASE_DIR, 'data', 'models', 'cnn')\n",
        "model_path = os.path.join(MODEL_DIR, 'food_classifier_best.keras')\n",
        "ONNX_OUTPUT_PATH = os.path.join(MODEL_DIR, 'food_classifier.onnx')\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('Converting model to ONNX format...')\n",
        "print('=' * 60)\n",
        "print(f'Input: {model_path}')\n",
        "print(f'Output: {ONNX_OUTPUT_PATH}')\n",
        "\n",
        "try:\n",
        "    import tf2onnx\n",
        "    import tempfile\n",
        "    import shutil\n",
        "    import json as json_module\n",
        "    import ast\n",
        "    import copy\n",
        "    import zipfile\n",
        "    \n",
        "    def _string_shape_to_list(s):\n",
        "        if not isinstance(s, str):\n",
        "            return None\n",
        "        s = s.strip()\n",
        "        inner = None\n",
        "        if s.startswith('TensorShape(') and s.endswith(')'):\n",
        "            inner = s[12:-1].strip()\n",
        "        if inner is None and '[' in s and ']' in s:\n",
        "            start, end = s.find('['), s.rfind(']')\n",
        "            if start != -1 and end != -1 and end > start:\n",
        "                inner = s[start:end+1]\n",
        "        if inner is None:\n",
        "            return None\n",
        "        inner = inner.replace('null', 'None').replace('Null', 'None')\n",
        "        try:\n",
        "            out = ast.literal_eval(inner)\n",
        "            return out if isinstance(out, list) else None\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def _string_to_list_any(s):\n",
        "        \"\"\"Parse any string that looks like a list/tuple into a list.\"\"\"\n",
        "        if not isinstance(s, str):\n",
        "            return None\n",
        "        s = s.strip()\n",
        "        if (s.startswith('[') and s.endswith(']')) or (s.startswith('(') and s.endswith(')')):\n",
        "            try:\n",
        "                out = ast.literal_eval(s)\n",
        "                if isinstance(out, (list, tuple)):\n",
        "                    return list(out)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "    \n",
        "    def _dtype_dict_to_str(dtype_val):\n",
        "        if not isinstance(dtype_val, dict):\n",
        "            return None\n",
        "        inner = dtype_val.get('config') or dtype_val\n",
        "        if isinstance(inner, dict) and 'name' in inner:\n",
        "            return inner['name']\n",
        "        return None\n",
        "    \n",
        "    def _fix_config_deep(obj):\n",
        "        \"\"\"Recursively fix config: shapes, dtypes, InputLayer params\"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            if obj.get('class_name') == 'InputLayer' and 'config' in obj:\n",
        "                c = obj['config']\n",
        "                if 'batch_shape' in c:\n",
        "                    c['batch_input_shape'] = c.pop('batch_shape')\n",
        "                c.pop('optional', None)\n",
        "            for k in list(obj.keys()):\n",
        "                v = obj[k]\n",
        "                if isinstance(v, str):\n",
        "                    parsed = _string_shape_to_list(v)\n",
        "                    if parsed is None:\n",
        "                        parsed = _string_to_list_any(v)\n",
        "                    if parsed is not None:\n",
        "                        obj[k] = parsed\n",
        "                elif k == 'dtype' and isinstance(v, dict):\n",
        "                    dtype_str = _dtype_dict_to_str(v)\n",
        "                    if dtype_str:\n",
        "                        obj[k] = dtype_str\n",
        "                else:\n",
        "                    _fix_config_deep(v)\n",
        "        elif isinstance(obj, list):\n",
        "            for i, item in enumerate(obj):\n",
        "                if isinstance(item, str):\n",
        "                    parsed = _string_shape_to_list(item)\n",
        "                    if parsed is None:\n",
        "                        parsed = _string_to_list_any(item)\n",
        "                    if parsed is not None:\n",
        "                        obj[i] = parsed\n",
        "                else:\n",
        "                    _fix_config_deep(item)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # APPROACH: Fix config.json inside .keras file, then load normally\n",
        "    # ========================================================================\n",
        "    print(\"\\n[1/4] Extracting and fixing model config...\")\n",
        "    \n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        # Extract .keras file\n",
        "        with zipfile.ZipFile(model_path, 'r') as zf:\n",
        "            zf.extractall(tmp_dir)\n",
        "        \n",
        "        # Load and fix config.json\n",
        "        config_path = os.path.join(tmp_dir, 'config.json')\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            config = json_module.load(f)\n",
        "        \n",
        "        # Deep fix all config issues\n",
        "        _fix_config_deep(config)\n",
        "        \n",
        "        # Also specifically fix inbound_nodes which may have string arrays\n",
        "        def fix_inbound_nodes(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                if 'inbound_nodes' in obj:\n",
        "                    nodes = obj['inbound_nodes']\n",
        "                    for i, node in enumerate(nodes):\n",
        "                        if isinstance(node, list):\n",
        "                            for j, item in enumerate(node):\n",
        "                                if isinstance(item, str):\n",
        "                                    parsed = _string_shape_to_list(item)\n",
        "                                    if parsed is None:\n",
        "                                        parsed = _string_to_list_any(item)\n",
        "                                    if parsed is not None:\n",
        "                                        nodes[i][j] = parsed\n",
        "                for v in obj.values():\n",
        "                    fix_inbound_nodes(v)\n",
        "            elif isinstance(obj, list):\n",
        "                for item in obj:\n",
        "                    fix_inbound_nodes(item)\n",
        "        \n",
        "        fix_inbound_nodes(config)\n",
        "        \n",
        "        # Save fixed config\n",
        "        with open(config_path, 'w', encoding='utf-8') as f:\n",
        "            json_module.dump(config, f)\n",
        "        \n",
        "        # Repack to new .keras file\n",
        "        fixed_keras_path = os.path.join(tmp_dir, 'fixed_model.keras')\n",
        "        with zipfile.ZipFile(fixed_keras_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "            for root, dirs, files in os.walk(tmp_dir):\n",
        "                for file in files:\n",
        "                    if file == 'fixed_model.keras':\n",
        "                        continue\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arc_name = os.path.relpath(file_path, tmp_dir)\n",
        "                    zf.write(file_path, arc_name)\n",
        "        \n",
        "        print(\"  OK: Config fixed and repacked\")\n",
        "        \n",
        "        # ========================================================================\n",
        "        # Load model from fixed .keras file\n",
        "        # ========================================================================\n",
        "        print(\"\\n[2/4] Loading fixed Keras model...\")\n",
        "        \n",
        "        # Custom InputLayer for compatibility\n",
        "        from tensorflow.keras.layers import InputLayer as KerasInputLayer\n",
        "        class InputLayerCompat(KerasInputLayer):\n",
        "            def __init__(self, batch_shape=None, optional=False, **kwargs):\n",
        "                if batch_shape is not None and 'batch_input_shape' not in kwargs:\n",
        "                    kwargs['batch_input_shape'] = batch_shape\n",
        "                super().__init__(**kwargs)\n",
        "        \n",
        "        custom_objs = {'InputLayer': InputLayerCompat}\n",
        "        \n",
        "        import warnings\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            model_for_onnx = tf.keras.models.load_model(\n",
        "                fixed_keras_path, \n",
        "                custom_objects=custom_objs, \n",
        "                compile=False, \n",
        "                safe_mode=False\n",
        "            )\n",
        "        \n",
        "        print(\"  OK: Model loaded!\")\n",
        "        print(f\"  Input: {model_for_onnx.input_shape}\")\n",
        "        print(f\"  Output: {model_for_onnx.output_shape}\")\n",
        "        \n",
        "        # ========================================================================\n",
        "        # Convert to ONNX\n",
        "        # ========================================================================\n",
        "        print(\"\\n[3/4] Converting to ONNX...\")\n",
        "        input_spec = [tf.TensorSpec((None, 224, 224, 3), tf.float32, name='input_1')]\n",
        "        onnx_model, _ = tf2onnx.convert.from_keras(model_for_onnx, input_signature=input_spec, opset=14)\n",
        "        \n",
        "        print(\"\\n[4/4] Saving ONNX model...\")\n",
        "        import onnx\n",
        "        onnx.save(onnx_model, ONNX_OUTPUT_PATH)\n",
        "    \n",
        "    print(f'\\nSUCCESS!')\n",
        "    print(f'  Output: {ONNX_OUTPUT_PATH}')\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f'ERROR: {e}')\n",
        "    print('  Install: pip install tf2onnx onnx')\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f'\\nERROR: {e}')\n",
        "\n",
        "print('=' * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
